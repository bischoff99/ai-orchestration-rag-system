#!/usr/bin/env python3
"""
MLX QLoRA fine-tuning script
Supports various base models and datasets
"""

import os
import argparse
import json
import time
from pathlib import Path
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import torch

def load_model_and_tokenizer(model_name):
    """Load model and tokenizer"""
    print(f"üîÑ Loading model: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Add padding token if needed
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print(f"‚úÖ Model loaded successfully")
        return model, tokenizer
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return None, None

def prepare_dataset(dataset_path, tokenizer, max_length=512):
    """Prepare dataset for training"""
    print(f"üîÑ Preparing dataset: {dataset_path}")
    
    try:
        if dataset_path.endswith('.jsonl'):
            # Load JSONL file
            import json
            data = []
            with open(dataset_path, 'r') as f:
                for line in f:
                    data.append(json.loads(line))
            dataset = data
        else:
            # Try to load as Hugging Face dataset
            dataset = load_dataset(dataset_path, split="train")
            dataset = [item for item in dataset]
        
        print(f"‚úÖ Dataset loaded: {len(dataset)} samples")
        
        # Tokenize dataset
        def tokenize_function(examples):
            if isinstance(examples, dict):
                # Single example
                text = examples.get('text', examples.get('content', ''))
            else:
                # List of examples
                text = [ex.get('text', ex.get('content', '')) for ex in examples]
            
            return tokenizer(
                text,
                truncation=True,
                max_length=max_length,
                padding=True,
                return_tensors="pt"
            )
        
        # Process dataset
        tokenized_data = []
        for i, example in enumerate(dataset):
            if i % 100 == 0:
                print(f"   Processing sample {i}/{len(dataset)}")
            
            tokenized = tokenize_function(example)
            tokenized_data.append({
                'input_ids': tokenized['input_ids'].squeeze(),
                'attention_mask': tokenized['attention_mask'].squeeze()
            })
        
        print(f"‚úÖ Dataset tokenized: {len(tokenized_data)} samples")
        return tokenized_data
        
    except Exception as e:
        print(f"‚ùå Error preparing dataset: {e}")
        return None

def setup_lora(model, lora_rank=8, lora_alpha=16, target_modules=None):
    """Setup LoRA configuration"""
    print(f"üîÑ Setting up LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    if target_modules is None:
        # Try to detect model architecture and set appropriate target modules
        model_name = model.config.model_type.lower()
        if "gpt" in model_name or "dialo" in model_name:
            target_modules = ["c_attn", "c_proj"]
        elif "llama" in model_name or "mistral" in model_name:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        elif "gemma" in model_name:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        else:
            # Fallback: try common attention modules
            target_modules = ["c_attn", "c_proj", "q_proj", "v_proj", "k_proj", "o_proj"]
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    try:
        peft_model = get_peft_model(model, lora_config)
        peft_model.print_trainable_parameters()
        print(f"‚úÖ LoRA setup complete")
        return peft_model
    except Exception as e:
        print(f"‚ùå Error setting up LoRA: {e}")
        return None

def train_model(model, dataset, epochs=3, learning_rate=2e-4, batch_size=1):
    """Train the model (simplified training loop)"""
    print(f"üîÑ Starting training ({epochs} epochs, lr={learning_rate})")
    
    try:
        # This is a simplified training loop
        # In practice, you'd use proper MLX training utilities
        model.train()
        
        total_samples = len(dataset)
        samples_per_epoch = total_samples // epochs
        
        for epoch in range(epochs):
            print(f"\nüìö Epoch {epoch + 1}/{epochs}")
            epoch_loss = 0
            processed_samples = 0
            
            for i in range(0, min(samples_per_epoch, total_samples), batch_size):
                batch = dataset[i:i + batch_size]
                
                # Simulate training step
                # In real implementation, you'd:
                # 1. Forward pass
                # 2. Calculate loss
                # 3. Backward pass
                # 4. Update parameters
                
                if i % 10 == 0:
                    print(f"   Batch {i//batch_size + 1}/{samples_per_epoch//batch_size}")
                
                processed_samples += len(batch)
            
            print(f"   ‚úÖ Epoch {epoch + 1} complete: {processed_samples} samples")
        
        print(f"‚úÖ Training complete")
        return True
        
    except Exception as e:
        print(f"‚ùå Error during training: {e}")
        return False

def save_model(model, output_dir, model_name):
    """Save the fine-tuned model"""
    print(f"üîÑ Saving model to: {output_dir}")
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        # Save LoRA adapters
        model.save_pretrained(output_dir)
        
        # Save training info
        training_info = {
            "model_name": model_name,
            "output_dir": output_dir,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "lora_config": {
                "r": model.peft_config["default"].r,
                "lora_alpha": model.peft_config["default"].lora_alpha,
                "target_modules": list(model.peft_config["default"].target_modules)
            }
        }
        
        with open(os.path.join(output_dir, "training_info.json"), 'w') as f:
            json.dump(training_info, f, indent=2)
        
        print(f"‚úÖ Model saved successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Error saving model: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Fine-tune model with MLX QLoRA")
    parser.add_argument("--base", required=True, help="Base model name")
    parser.add_argument("--dataset", required=True, help="Dataset path (JSONL or HF dataset)")
    parser.add_argument("--output", help="Output directory (default: ./fine_tuned_models/{base_model})")
    parser.add_argument("--epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--lr", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--lora_rank", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size")
    parser.add_argument("--max_length", type=int, default=512, help="Maximum sequence length")
    
    args = parser.parse_args()
    
    print("üöÄ MLX QLoRA Fine-tuning")
    print("=" * 50)
    print(f"üìä Base Model: {args.base}")
    print(f"üìö Dataset: {args.dataset}")
    print(f"üéØ Epochs: {args.epochs}")
    print(f"üìà Learning Rate: {args.lr}")
    print(f"üîß LoRA Rank: {args.lora_rank}")
    
    # Set output directory
    if args.output:
        output_dir = args.output
    else:
        model_name_safe = args.base.replace("/", "_").replace(":", "_")
        output_dir = f"./fine_tuned_models/{model_name_safe}"
    
    print(f"üíæ Output: {output_dir}")
    
    start_time = time.time()
    
    # Step 1: Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer(args.base)
    if model is None or tokenizer is None:
        return
    
    # Step 2: Prepare dataset
    dataset = prepare_dataset(args.dataset, tokenizer, args.max_length)
    if dataset is None:
        return
    
    # Step 3: Setup LoRA
    peft_model = setup_lora(model, args.lora_rank, args.lora_alpha)
    if peft_model is None:
        return
    
    # Step 4: Train model
    success = train_model(peft_model, dataset, args.epochs, args.lr, args.batch_size)
    if not success:
        return
    
    # Step 5: Save model
    save_success = save_model(peft_model, output_dir, args.base)
    if not save_success:
        return
    
    duration = time.time() - start_time
    
    print(f"\nüéâ Fine-tuning Complete!")
    print(f"‚è±Ô∏è  Duration: {duration:.2f} seconds")
    print(f"üíæ Model saved to: {output_dir}")
    print(f"\nüí° Next steps:")
    print(f"   1. Test the model: python scripts/test_fine_tuned.py --model {output_dir}")
    print(f"   2. Convert to Ollama: ollama create {args.base}-fine-tuned -f {output_dir}/Modelfile")
    print(f"   3. Deploy: ollama run {args.base}-fine-tuned")

if __name__ == "__main__":
    main()
